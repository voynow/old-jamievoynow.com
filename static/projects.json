[
    {
        "title": "turbo-docs",
        "description": "GPT-powered development tool for generating documentation",
        "url": "https://github.com/voynow/turbo-docs",
        "overview": "The \"turbo_docs\" Python package is designed to automate and streamline the process of generating documentation for software projects. It uses the advanced capabilities of the OpenAI's GPT-4 model to automatically generate high-quality README files and Python docstrings.\n\nThe package includes several core features:\n\nAutomatic README Generation:\nThe package can generate a comprehensive README.md file for a project, ensuring that all aspects of the project are well-documented.\n\nAutomatic Docstring Generation:\nIt also automatically generates docstrings for Python functions in a project, helping maintain clean and well-documented code.\n\nCustomizability:\nUsers can specify files to exclude from the documentation process, providing flexibility in what gets documented.\n\nEase of Use:\nThe package provides simple command-line options to trigger the different functionalities, making it easy to integrate into the development workflow.\n\nThe turbo_docs package is a robust solution for developers aiming to improve their project documentation while saving time and effort.",
        "code_example": "# First, set your OpenAI API key as an environment variable:\nexport OPENAI_API_KEY=<your_api_key>\n\n# Navigate to the directory you want to generate documentation for:\ncd <path-to-your-project-directory>\n\n# In your project directoryn install turbo_docs:\npip install turbo_docs\n\n# Once installed, you can run `turbo_docs` with the desired command-line options.\n\n# To generate a README:\nturbo_docs --readme\n\n# To generate docstrings:\nturbo_docs --docstring\n\n# To copy the directory text to the clipboard:\nturbo_docs --copy"
    },
    {
        "title": "repo-chat",
        "description": "Using GPT, Langchain, and Pinecone to chat with a GitHub repository",
        "url": "https://github.com/voynow/repo-chat",
        "overview": "This Python package is designed to facilitate chat interactions with a large language model using document retrieval from a GitHub repository. It leverages the langchain library, which provides functionalities for managing chains and openai credentials.\n\nThe package consists of several modules:\n\nchain_manager.py:\nThis module defines a class called CustomChain that manages the creation and storage of chains and OpenAI credentials. It provides methods to create an LLMChain given a model, input variables, template, and temperature. The class also has a __call__ method to call the chain with input data. The get_chain function returns a CustomChain instance based on the specified template type.\n\nchat_utils.py:\nThis module contains the RetrievalChain class, which enables chat interactions with a large language model using document retrieval. It uses the langchain library to facilitate the retrieval and chat operations. The class initializes with a vectorstore and a repository. It provides methods for logging entries, retrieving chain inputs based on a query, calling a chain and logging the execution time, processing a query using context validation and running the query chains, iterating through queries to find an answer, and managing the workflow for upgrading and processing the query. The chat method initiates the chat with the QA chain.\n\ncustom_loaders.py:\nThis module defines a TurboGitLoader class, which is responsible for loading files from a Git repository into a list of documents in parallel. It uses the git library for repository operations and provides a method to load all files from the repository. The module also includes other utility classes and functions related to document loading.\n\neval_utils.py:\nThis module contains classes for evaluating the performance of the RetrievalChain using the CriticChain to score responses. The CriticChain class is used for scoring a given response to a query, while the QueryEvaluator class evaluates the RetrievalChain by scoring responses for a given query and repository. The MultiQueryEvaluator class performs parallel evaluation for multiple queries. The module also includes functions for flattening the evaluation responses into dataframes.\n\ngit2vectors.py:\nThis module provides functionality for converting a Git repository into a vectorstore. It includes functions for loading data from a Git repository, splitting text into chunks, embedding and upserting data into a Pinecone index, and creating a vectorstore.\n\ntemplates.py:\nThis module defines templates used for generating prompts in the chat process. It includes templates for upgrading queries, running queries, validating context, and scoring responses.\n\nOverall, this package offers a convenient and efficient way to interact with a large language model using document retrieval from a GitHub repository, making it suitable for various natural language processing tasks.",
        "code_example": "from repo_chat.chat_utils import RetrievalChain\nfrom repo_chat.git2vectors import create_vectorstore\n    \n# Define the GitHub repository URL\nrepo_url = \"https://github.com/example_user/example_repo\"\n\n# Create a vectorstore from the GitHub repository\ncreate_vectorstore(repo_url)\n\n# Initialize the RetrievalChain\nretrieval_chain = RetrievalChain(get_vectorstore(), repo_url)\n\n# Start a chat session\nquery = \"How do I use the 'run' function?\"\nresponse = retrieval_chain.chat(query)\n\n# Print the response from the chat session\nprint(\"Chat Response:\")\nprint(response[\"text\"])\n\n# Retrieve the similar documents for a query\nsimilar_docs = retrieval_chain.get_chain_inputs(query)[\"similar_documents\"]\n\n# for more details check out the .ipynb files on github"
    },
    {
        "title": "strava",
        "description": "[Decommissioned] Custom data app built on the strava API test",
        "url": "https://github.com/voynow/strava",
        "overview": "In this repository, there are two major components built for AWS Lambda functions:\n\n    The dashboard_lambda component:\n    This component is built to visualize and update a dashboard that displays some statistics derived from Strava data. It fetches and processes the data (stored on an AWS S3 bucket) in order to calculate moving averages and prepare a heatmap. Then, it generates a dashboard using Matplotlib and updates the dashboard on S3.\n\n    The ingestion_lambda component:\n    This component is responsible for data ingestion. It communicates with the Strava API to fetch activities data and other related data. It then stores this data in an AWS S3 bucket. The Strava API access token is fetched by interacting with the OAuth interface using a Selenium-based solution.\n\n    Both components rely on AWS services like S3 for data storage and Secrets Manager for storing sensitive data such as API keys and credentials.",
        "code_example": "# Example code from the ingestion lambda\n\n# Import the necessary modules\nimport utils.strava_api as strava_api\nimport utils.s3ops as s3ops\n\n# Define the Lambda handler function\ndef lambda_handler(event, context):\n    # Retrieve the access token using Strava API\n    access_token = strava_api.get_access_token()\n\n    # Update the activities in the S3 bucket\n    s3ops.update_activities(access_token)\n    s3ops.update_tables(access_token)\n    return 1\n\n# Example code from the dashboard lambda\n\n# Import the necessary modules\nfrom utils import data, dashboard\n\n# Define the Lambda handler function\ndef lambda_handler(event, context):\n\n    # Retrieve the data from S3\n    df = data.calc_moving_average()\n    heatmap = data.get_philly_heatmap()\n\n    # Update the dashboard\n    dashboard.update_dashboard(df, heatmap)\n    return 1"
    }
]
